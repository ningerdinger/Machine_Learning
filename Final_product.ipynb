{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f072c436",
   "metadata": {},
   "source": [
    "# WEO Economic Data Analysis & Recession Prediction\n",
    "\n",
    "**Objective:** Load World Economic Outlook (WEO) data, clean and transform it, then use machine learning models to predict global recessions.\n",
    "\n",
    "**Workflow:**\n",
    "1. Data loading and cleaning\n",
    "2. Feature engineering and recession flagging\n",
    "3. Exploratory data analysis\n",
    "4. Model training with full and reduced feature sets (comparing 13 vs 5 features)\n",
    "5. Economy-specific analysis (Upper vs Lower economies with both feature sets)\n",
    "6. Future predictions for all scenarios\n",
    "\n",
    "**Models Used:** Logistic Regression, Random Forest, Gradient Boosting, Linear SVM, KNN, Naive Bayes, MLP, Decision Tree, and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# Machine learning models\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# File handling\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "# Optional pycountry for continent mapping\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    HAS_PYCOUNTRY = True\n",
    "except ImportError:\n",
    "    HAS_PYCOUNTRY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5987b26",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# Detect encoding and delimiter\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a530cd",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Transformation\n",
    "\n",
    "## Filter to Selected Economic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be117543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\", \"Country/Series-specific Notes\", \"Subject Notes\", \n",
    "                 \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\"], inplace=True)\n",
    "\n",
    "codes = {\n",
    "    # Core growth & external\n",
    "    \"NGDP_RPCH\", \"NGDPRPC\", \"PCPIPCH\", \"TX_RPCH\", \"TM_RPCH\", \"BCA_NGDPD\",\n",
    "    # Fiscal & debt aggregates\n",
    "    \"GGR_NGDP\", \"GGX_NGDP\", \"GGXWDN_NGDP\", \"GGXWDG_NGDP\",\n",
    "    # Savings & investment\n",
    "    \"NGSD_NGDP\", \"NID_NGDP\",\n",
    "    # Prices\n",
    "    \"PCPI\"\n",
    "}\n",
    "\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010051",
   "metadata": {},
   "source": [
    "## Data Reshaping: Wide to Long to Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = df.columns[2:]\n",
    "\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1eba",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "## Add Recession Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Define IMF-recognized global recession years ---\n",
    "global_recession_years = [1982, 1991, 2009, 2020]\n",
    "\n",
    "# --- Step 2: Ensure chronological order ---\n",
    "df_pivot = df_pivot.sort_index()\n",
    "\n",
    "# --- Step 3a: GDP-based local recession flag (two consecutive annual declines)\n",
    "flag_gdp = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3c: Investment collapse\n",
    "flag_invest = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3d: Savings decline\n",
    "flag_savings = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3e: Trade shock (both exports and imports decline)\n",
    "flag_trade = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3f: Fiscal stress (large deficit + rising debt)\n",
    "flag_fiscal = (\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXCNL_NGDP\"].transform(lambda x: x < -5)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDN_NGDP\"].transform(lambda x: x.diff() > 3))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3g: Inflation shock (stagflation scenario)\n",
    "flag_inflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3h: Unified local recession flag\n",
    "local_recession_flag = (\n",
    "    flag_gdp | flag_unemp | flag_invest | flag_savings |\n",
    "    flag_trade | flag_fiscal | flag_inflation\n",
    ")\n",
    "\n",
    "# --- Step 3i: Combine with global recession years\n",
    "df_pivot[\"Recession\"] = (\n",
    "    df_pivot.index.isin(global_recession_years).astype(int) | local_recession_flag\n",
    ")\n",
    "\n",
    "# --- Step 4: Clean dataset ---\n",
    "df_pivot = df_pivot.dropna().sort_index(ascending=True)\n",
    "\n",
    "# --- Preview ---\n",
    "df_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global recession years\n",
    "global_recession_years = [1982, 1991, 2009, 2020]\n",
    "\n",
    "# Filter all rows where Recession == 1 but year not in global list\n",
    "local_only_recessions = df_pivot[\n",
    "    (df_pivot[\"Recession\"] == 1) & \n",
    "    (~df_pivot.index.isin(global_recession_years))\n",
    "]\n",
    "local_only_recessions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b020",
   "metadata": {},
   "source": [
    "## Review Remaining Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1820ea",
   "metadata": {},
   "source": [
    "## Split Training and Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d14c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_pivot.loc[df_pivot.index > 2024]\n",
    "df_predict_original = df_predict.copy()\n",
    "df_predict = df_predict.drop(columns=[\"Recession\", \"Country\"])\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[df_pivot.index <= 2024]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37a376",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_filtered.drop(columns=[\"Country\", \"Recession\"]).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b2f5",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=[\"Recession\", \"Country\"])\n",
    "y = df_filtered[\"Recession\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccf90",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models\n",
    "\n",
    "## Global Dataset - Full Features (13 Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb849a40",
   "metadata": {},
   "source": [
    "### Define and Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8774c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  TRAINING FUNCTION with SMOTE + Metrics + Confusion Matrix\n",
    "# ============================================================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from xgboost import XGBClassifier   # uncomment if you use XGBoost\n",
    "\n",
    "def train_all_models(X_train, y_train, X_test, y_test, model_params=None, use_xgb=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Train multiple models with SMOTE, evaluate them on train/test sets,\n",
    "    and return metrics including confusion matrices. Threshold can be set\n",
    "    for probability-based classifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default hyperparameters\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'logit': {\n",
    "                'C': 0.2, 'penalty': 'l2', 'solver': 'lbfgs',\n",
    "                'max_iter': 5000, 'random_state': 42\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': 200, 'max_depth': 4,\n",
    "                'min_samples_leaf': 20, 'min_samples_split': 20,\n",
    "                'max_features': 0.3, 'random_state': 42\n",
    "            },\n",
    "            'gb': {\n",
    "                'n_estimators': 200, 'learning_rate': 0.03,\n",
    "                'max_depth': 2, 'min_samples_leaf': 20,\n",
    "                'subsample': 0.6, 'random_state': 42\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': 3, 'min_samples_leaf': 30, 'random_state': 42\n",
    "            },\n",
    "            'xgb': {\n",
    "                'n_estimators': 200, 'learning_rate': 0.05,\n",
    "                'max_depth': 2, 'subsample': 0.7, 'colsample_bytree': 0.6,\n",
    "                'reg_alpha': 0.4, 'reg_lambda': 2.0, 'random_state': 42,\n",
    "                'use_label_encoder': False, 'eval_metric': 'logloss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Train models -----------------------------------------------------\n",
    "    logit = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"logit\", LogisticRegression(**model_params['logit']))\n",
    "    ])\n",
    "    logit.fit(X_train, y_train)\n",
    "\n",
    "    rf = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"rf\", RandomForestClassifier(**model_params['rf']))\n",
    "    ])\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    gb = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"gb\", GradientBoostingClassifier(**model_params['gb']))\n",
    "    ])\n",
    "    gb.fit(X_train, y_train)\n",
    "\n",
    "    dt = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"dt\", DecisionTreeClassifier(**model_params['dt']))\n",
    "    ])\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": logit,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Gradient Boosting\": gb,\n",
    "        \"Decision Tree\": dt,\n",
    "    }\n",
    "\n",
    "    # XGBoost optional\n",
    "    if use_xgb:\n",
    "        xgb = ImbPipeline([\n",
    "            (\"smote\", SMOTE(random_state=42)),\n",
    "            (\"xgb\", XGBClassifier(**model_params['xgb']))\n",
    "        ])\n",
    "        xgb.fit(X_train, y_train)\n",
    "        models[\"XGBoost\"] = xgb\n",
    "\n",
    "    # Ensemble (soft voting only)\n",
    "    ensemble_estimators = [\n",
    "        (\"logit\", logit.named_steps[\"logit\"]),\n",
    "        (\"rf\", rf.named_steps[\"rf\"]),\n",
    "        (\"gb\", gb.named_steps[\"gb\"])\n",
    "    ]\n",
    "    if use_xgb:\n",
    "        ensemble_estimators.append((\"xgb\", xgb.named_steps[\"xgb\"]))\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=ensemble_estimators, voting=\"soft\")\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    models[\"Ensemble\"] = ensemble\n",
    "\n",
    "    # Compute metrics -----------------------------------------------------------\n",
    "    results = {}\n",
    "    confusion_mats = {}\n",
    "\n",
    "    for name, m in models.items():\n",
    "        if hasattr(m, \"predict_proba\"):\n",
    "            y_pred_train = (m.predict_proba(X_train)[:, 1] >= threshold).astype(int)\n",
    "            y_pred_test = (m.predict_proba(X_test)[:, 1] >= threshold).astype(int)\n",
    "        else:\n",
    "            y_pred_train = m.predict(X_train)\n",
    "            y_pred_test = m.predict(X_test)\n",
    "\n",
    "        results[name] = {\n",
    "            \"Train Accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "            \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "            \"Precision\": precision_score(y_test, y_pred_test, zero_division=0),\n",
    "            \"Recall\": recall_score(y_test, y_pred_test, zero_division=0),\n",
    "            \"F1\": f1_score(y_test, y_pred_test, zero_division=0)\n",
    "        }\n",
    "\n",
    "        confusion_mats[name] = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    return models, results_df, confusion_mats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             FEATURE IMPORTANCE PLOTTING\n",
    "# ============================================================\n",
    "def plot_feature_importance(models, feature_names, title_prefix=\"\"):\n",
    "    logit = models.get(\"Logistic Regression\")\n",
    "    rf = models.get(\"Random Forest\")\n",
    "    gb = models.get(\"Gradient Boosting\")\n",
    "    dt = models.get(\"Decision Tree\")\n",
    "\n",
    "    coef = logit.named_steps['logit'].coef_[0]\n",
    "    logit_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Coefficient\": coef,\n",
    "        \"Abs_Importance\": np.abs(coef)\n",
    "    }).sort_values(\"Abs_Importance\")\n",
    "\n",
    "    rf_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": rf.named_steps['rf'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    gb_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": gb.named_steps['gb'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    dt_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": dt.named_steps['dt'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    axes[0, 0].barh(logit_importance[\"Feature\"], logit_importance[\"Coefficient\"])\n",
    "    axes[0, 0].set_title(f\"{title_prefix}Logistic Regression\")\n",
    "\n",
    "    axes[0, 1].barh(rf_importance[\"Feature\"], rf_importance[\"Importance\"])\n",
    "    axes[0, 1].set_title(f\"{title_prefix}Random Forest\")\n",
    "\n",
    "    axes[1, 0].barh(gb_importance[\"Feature\"], gb_importance[\"Importance\"])\n",
    "    axes[1, 0].set_title(f\"{title_prefix}Gradient Boosting\")\n",
    "\n",
    "    axes[1, 1].barh(dt_importance[\"Feature\"], dt_importance[\"Importance\"])\n",
    "    axes[1, 1].set_title(f\"{title_prefix}Decision Tree\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "def make_predictions(models, df_predict, use_threshold=True, threshold=0.20):\n",
    "    \"\"\"\n",
    "    Return predictions from every model in one dataframe.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        if use_threshold and hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(df_predict)[:, 1]\n",
    "            predictions[name] = (proba >= threshold).astype(int)\n",
    "        else:\n",
    "            predictions[name] = model.predict(df_predict)\n",
    "    return pd.DataFrame(predictions, index=df_predict.index)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             CONFUSION MATRIX DISPLAY FUNCTION (with metrics incl. Accuracy)\n",
    "# ============================================================\n",
    "def show_confusion_matrices(confusion_mats, results_df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Display confusion matrices for all models side by side,\n",
    "    with Accuracy, Precision, Recall, and F1 shown under each matrix.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "    n_models = len(confusion_mats)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (name, cm) in zip(axes, confusion_mats.items()):\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=\"Blues\", ax=ax, colorbar=False)\n",
    "\n",
    "        # Extract metrics from results_df\n",
    "        accuracy = results_df.loc[name, \"Test Accuracy\"]\n",
    "        precision = results_df.loc[name, \"Precision\"]\n",
    "        recall = results_df.loc[name, \"Recall\"]\n",
    "        f1 = results_df.loc[name, \"F1\"]\n",
    "\n",
    "        ax.set_title(f\"{name}\\n(threshold={threshold})\")\n",
    "        ax.set_xlabel(\n",
    "            f\"Acc={accuracy:.2f}, Prec={precision:.2f}, Rec={recall:.2f}, F1={f1:.2f}\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a73d",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, summary_df, confusion_mats = train_all_models(X_train, y_train, X_test, y_test)\n",
    "print(summary_df)\n",
    "print(confusion_mats[\"Ensemble\"])  # example: confusion matrix for ensemble\n",
    "\n",
    "plot_feature_importance(models, X_train.columns.tolist())\n",
    "\n",
    "predictions = make_predictions(models, df_predict)\n",
    "print(predictions.head())\n",
    "\n",
    "# Show confusion matrices with metrics underneath\n",
    "show_confusion_matrices(confusion_mats, summary_df, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b66c25",
   "metadata": {},
   "source": [
    "### Reduced Global Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['TM_RPCH', 'GGXONLB_NGDP', 'TX_RPCH', 'GGXCNL_NGDP', 'PCPI']\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "# Unpack all three return values\n",
    "models_reduced, summary_df_reduced, confusion_mats_reduced = train_all_models(\n",
    "    X_train_reduced, y_train, X_test_reduced, y_test, threshold=0.5\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plot_feature_importance(models_reduced, feature_names=selected_features, title_prefix=\"Reduced Features - \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cba79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on restricted df\n",
    "df_predict_restricted = df_predict[selected_features]\n",
    "predictions_restricted = make_predictions(models_reduced, df_predict_restricted, threshold=0.5)\n",
    "print(predictions_restricted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e89390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_reduced, summary_df_reduced, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12134a9a",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8430911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map countries to continents (same logic as before)\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        try:\n",
    "            lookup_name = name.replace('_', ' ')\n",
    "            country = pycountry.countries.lookup(lookup_name)\n",
    "            alpha2 = country.alpha_2\n",
    "            cc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "            continent_map = {\n",
    "                'AF': 'Africa',\n",
    "                'AS': 'Asia',\n",
    "                'EU': 'Europe',\n",
    "                'NA': 'North_America',\n",
    "                'OC': 'Oceania',\n",
    "                'SA': 'South_America'\n",
    "            }\n",
    "            return continent_map.get(cc, 'Unknown')\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "except ImportError:\n",
    "    # Fallback mapping for common countries (extend as needed)\n",
    "    fallback = {\n",
    "        'United_States': 'North_America', 'Canada': 'North_America', 'Mexico': 'North_America',\n",
    "        'China': 'Asia', 'India': 'Asia', 'Japan': 'Asia', 'Afghanistan': 'Asia',\n",
    "        'Korea': 'Asia', 'Indonesia': 'Asia', 'Thailand': 'Asia', 'Vietnam': 'Asia',\n",
    "        'Germany': 'Europe', 'France': 'Europe', 'United_Kingdom': 'Europe', 'Italy': 'Europe',\n",
    "        'Spain': 'Europe', 'Russia': 'Europe', 'Turkey': 'Europe', 'Poland': 'Europe',\n",
    "        'Brazil': 'South_America', 'Argentina': 'South_America', 'Chile': 'South_America',\n",
    "        'Colombia': 'South_America', 'Peru': 'South_America', 'Venezuela': 'South_America',\n",
    "        'Australia': 'Oceania', 'New_Zealand': 'Oceania',\n",
    "        'South_Africa': 'Africa', 'Nigeria': 'Africa', 'Egypt': 'Africa', 'Zimbabwe': 'Africa',\n",
    "        'Kenya': 'Africa', 'Ethiopia': 'Africa', 'Morocco': 'Africa',\n",
    "        # Additional countries...\n",
    "        'Albania': 'Europe', 'Algeria': 'Africa', 'Austria': 'Europe', 'Barbados': 'North_America',\n",
    "        'Belgium': 'Europe', 'Bolivia': 'South_America', 'Bosnia_and_Herzegovina': 'Europe',\n",
    "        'Bulgaria': 'Europe', 'Cabo_Verde': 'Africa', 'Costa_Rica': 'North_America',\n",
    "        'Croatia': 'Europe', 'Cyprus': 'Europe', 'Czech_Republic': 'Europe', 'Denmark': 'Europe',\n",
    "        'Dominican_Republic': 'North_America', 'Estonia': 'Europe', 'Finland': 'Europe',\n",
    "        'Hungary': 'Europe', 'Iceland': 'Europe', 'Ireland': 'Europe',\n",
    "        'Islamic_Republic_of_Iran': 'Asia', 'Israel': 'Asia', 'Jordan': 'Asia',\n",
    "        'Kazakhstan': 'Asia', 'Latvia': 'Europe', 'Lebanon': 'Asia', 'Lithuania': 'Europe',\n",
    "        'Luxembourg': 'Europe', 'Malta': 'Europe', 'Netherlands': 'Europe',\n",
    "        'North_Macedonia': 'Europe', 'Norway': 'Europe', 'Pakistan': 'Asia',\n",
    "        'Panama': 'North_America', 'Paraguay': 'South_America', 'Portugal': 'Europe',\n",
    "        'Romania': 'Europe', 'Saudi_Arabia': 'Asia', 'Serbia': 'Europe', 'Seychelles': 'Africa',\n",
    "        'Slovak_Republic': 'Europe', 'Slovenia': 'Europe', 'Sweden': 'Europe',\n",
    "        'Switzerland': 'Europe', 'Syria': 'Asia', 'Taiwan_Province_of_China': 'Asia',\n",
    "        'Trinidad_and_Tobago': 'North_America', 'Türkiye': 'Europe', 'Uruguay': 'South_America'\n",
    "    }\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        return fallback.get(name.replace(' ', '_'), 'Unknown')\n",
    "\n",
    "# --- Add Continent column ---\n",
    "df_filtered_copy = df_pivot.copy()\n",
    "df_filtered_copy['Continent'] = df_filtered_copy['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "# --- Map continents to economy groups ---\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_filtered_copy['EconomyGroup'] = df_filtered_copy['Continent'].map(continent_to_economy)\n",
    "\n",
    "# --- Create Lower and Upper economy DataFrames ---\n",
    "df_Lower_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Lower_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "df_Upper_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Upper_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"Created economy-specific DataFrames:\")\n",
    "print(f\" - Lower_Economies: df_Lower_Economies (rows: {len(df_Lower_Economies)})\")\n",
    "print(f\" - Upper_Economies: df_Upper_Economies (rows: {len(df_Upper_Economies)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a986416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lower_Economies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03096565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Upper_Economies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8748543",
   "metadata": {},
   "source": [
    "## Split Dataset of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_original['Continent'] = df_predict_original['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_predict_original['EconomyGroup'] = df_predict_original['Continent'].map(continent_to_economy)\n",
    "\n",
    "df_predict_lower = df_predict_original[df_predict_original['EconomyGroup'] == 'Lower_Economies'].drop(\n",
    "    columns=['Continent', 'EconomyGroup', 'Country']\n",
    ")\n",
    "df_predict_upper = df_predict_original[df_predict_original['EconomyGroup'] == 'Upper_Economies'].drop(\n",
    "    columns=['Continent', 'EconomyGroup', 'Country']\n",
    ")\n",
    "\n",
    "print(\"Created economy-specific prediction DataFrames from df_predict_original:\")\n",
    "print(f\" - Lower_Economies predictions: {len(df_predict_lower)} rows\")\n",
    "print(f\" - Upper_Economies predictions: {len(df_predict_upper)} rows\")\n",
    "\n",
    "df_predict_lower.head()\n",
    "df_predict_upper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989b926",
   "metadata": {},
   "source": [
    "# 6. Economy-Specific Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeadb0",
   "metadata": {},
   "source": [
    "## Upper Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19751b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_Upper_Economies.drop(columns=[\"Recession\", \"Country\"])\n",
    "y = df_Upper_Economies[\"Recession\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_upper, summary_df_upper, confusion_mats_upper = train_all_models(\n",
    "    X_train, y_train, X_test, y_test, threshold=0.5\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_upper)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_upper, X_train.columns.tolist(), title_prefix=\"Upper Economies - \")\n",
    "\n",
    "# Predictions on restricted df\n",
    "X_predict_upper = df_predict_upper.drop(columns=[\"Recession\", \"Country\"], errors='ignore')\n",
    "predictions_upper = make_predictions(models_upper, X_predict_upper, threshold=0.5)\n",
    "print(predictions_upper)\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper, summary_df_upper, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b3137",
   "metadata": {},
   "source": [
    "## Lower Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Lower Economies\n",
    "X = df_Lower_Economies.drop(columns=[\"Recession\", \"Country\"])\n",
    "y = df_Lower_Economies[\"Recession\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_lower, summary_df_lower, confusion_mats_lower = train_all_models(\n",
    "    X_train, y_train, X_test, y_test, threshold=0.5\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_lower)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_lower, X_train.columns.tolist(), title_prefix=\"Lower Economies - \")\n",
    "\n",
    "# Predictions on restricted df\n",
    "X_predict_lower = df_predict_lower.drop(columns=[\"Recession\", \"Country\"], errors='ignore')\n",
    "predictions_lower = make_predictions(models_lower, X_predict_lower, threshold=0.5)\n",
    "predictions_lower.head()\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower, summary_df_lower, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2538",
   "metadata": {},
   "source": [
    "## Upper Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['TM_RPCH', 'GGXONLB_NGDP', 'TX_RPCH', 'GGXCNL_NGDP', 'PCPI']\n",
    "\n",
    "X_upper = df_Upper_Economies[selected_features]\n",
    "y_upper = df_Upper_Economies[\"Recession\"]\n",
    "\n",
    "split_index_upper = int(len(X_upper) * 0.8)\n",
    "X_train_upper = X_upper.iloc[:split_index_upper]\n",
    "X_test_upper = X_upper.iloc[split_index_upper:]\n",
    "y_train_upper = y_upper.iloc[:split_index_upper]\n",
    "y_test_upper = y_upper.iloc[split_index_upper:]\n",
    "\n",
    "# ✅ Unpack all three return values\n",
    "models_upper, summary_df_upper, confusion_mats_upper = train_all_models(\n",
    "    X_train_upper, y_train_upper, X_test_upper, y_test_upper, threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Upper Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_upper)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(models_upper, feature_names=selected_features, \n",
    "                        title_prefix=\"Upper Economies - Reduced Features - \")\n",
    "\n",
    "# Predictions\n",
    "X_predict_upper_reduced = df_predict_upper[selected_features]\n",
    "predictions_upper_reduced = make_predictions(models_upper, X_predict_upper_reduced, threshold=0.5)\n",
    "print(\"Predictions for Upper Economies (Reduced Features):\")\n",
    "print(predictions_upper_reduced)\n",
    "\n",
    "# ✅ Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper, summary_df_upper, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f45978",
   "metadata": {},
   "source": [
    "## Lower Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower = df_Lower_Economies[selected_features]\n",
    "y_lower = df_Lower_Economies[\"Recession\"]\n",
    "\n",
    "split_index_lower = int(len(X_lower) * 0.8)\n",
    "X_train_lower = X_lower.iloc[:split_index_lower]\n",
    "X_test_lower = X_lower.iloc[split_index_lower:]\n",
    "y_train_lower = y_lower.iloc[:split_index_lower]\n",
    "y_test_lower = y_lower.iloc[split_index_lower:]\n",
    "\n",
    "# ✅ Unpack all three return values\n",
    "models_lower, summary_df_lower, confusion_mats_lower = train_all_models(\n",
    "    X_train_lower, y_train_lower, X_test_lower, y_test_lower, threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"Lower Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_lower)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(models_lower, feature_names=selected_features, \n",
    "                        title_prefix=\"Lower Economies - Reduced Features - \")\n",
    "\n",
    "# Predictions\n",
    "X_predict_lower_reduced = df_predict_lower[selected_features]\n",
    "predictions_lower_reduced = make_predictions(models_lower, X_predict_lower_reduced, threshold=0.5)\n",
    "print(\"Predictions for Lower Economies (Reduced Features):\")\n",
    "print(predictions_lower_reduced)\n",
    "\n",
    "# ✅ Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower, summary_df_lower, threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
