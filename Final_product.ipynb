{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f072c436",
   "metadata": {},
   "source": [
    "# WEO data cleaning and reshaping\n",
    "\n",
    "This notebook loads WEO data, filters to selected indicators, cleans numeric values, pivots to a panel (Country Ã— Year) with indicators as columns, adds a global recession flag, drops incomplete years, and writes a cleaned CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# detect encoding (fallback to latin-1) and sample for delimiter sniffing\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count columns\n",
    "print(\"Number of columns:\", df.shape[1])\n",
    "\n",
    "# list column names\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# counts of non-null values per column (good for spotting missing data)\n",
    "print(df.count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a530cd",
   "metadata": {},
   "source": [
    "Lets drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be117543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\",\"Country/Series-specific Notes\", \"Subject Notes\", \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\" ], inplace=True)\n",
    "\n",
    "codes = {\"NGDPD\",\"NGSD_NGDP\",\"PCPI\",\"TM_RPCH\",\"TX_RPCH\",\"LUR\",\"LP\",\"GGR_NGDP\",\"GGX_NGDP\",\"GGXCNL_NGDP\",\"GGSB_NPGDP\",\"GGXONLB_NGDP\",\"GGXWDN_NGDP\",\"BCA_NGDPD\"}\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010051",
   "metadata": {},
   "source": [
    "Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Identify year columns ---\n",
    "year_cols = df.columns[2:]  # after WEO Subject Code and Country\n",
    "\n",
    "# --- Step 2: Clean numeric formatting ---\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# --- Step 3: Clean country names ---\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "# --- Step 4: Melt into long format ---\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "# --- Step 5: Keep only valid year rows ---\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "# --- Step 6: Clean numeric values ---\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# --- Step 7: Pivot using WEO Subject Code as columns ---\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",   # use NGDPD, PCPI, etc.\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# --- Step 8: Finalize ---\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1eba",
   "metadata": {},
   "source": [
    "# Add recession years and drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year index to integer\n",
    "df_pivot.index = df_pivot.index.astype(int)\n",
    "\n",
    "# Define global recession years\n",
    "global_recession_years = [\n",
    "    1981, 1982, 1983,\n",
    "    1990, 1991, 1992, 1993,\n",
    "    2008, 2009, 2010,\n",
    "    2020, 2021\n",
    "]\n",
    "\n",
    "# Add Global_Recession column\n",
    "df_pivot[\"Global_Recession\"] = df_pivot.index.isin(global_recession_years).astype(int)\n",
    "\n",
    "# Remove rows with any NaN values\n",
    "df_pivot = df_pivot.dropna()\n",
    "df_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b020",
   "metadata": {},
   "source": [
    "Lets take a look at which countries remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1820ea",
   "metadata": {},
   "source": [
    "Filter years up and include 2024. Also leave a set of prediction years of 2025 and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d14c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_pivot.loc[df_pivot.index > 2024]\n",
    "df_predict = df_predict.drop(columns=[\"Global_Recession\", \"Country\"])\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[df_pivot.index <= 2024]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37a376",
   "metadata": {},
   "source": [
    "Lets make a heatmap and see how well data is correlated with eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Exploratory Plots ---\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df_filtered.drop(columns=[\"Country\"]).corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap of Economic Indicators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b2f5",
   "metadata": {},
   "source": [
    "create and a x and y dataset so that we can train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_filtered is your dataframe with Year as index\n",
    "X = df_filtered.drop(columns=[\"Global_Recession\", \"Country\"])  # features\n",
    "y = df_filtered[\"Global_Recession\"]                           # target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccf90",
   "metadata": {},
   "source": [
    "# Modelling portion on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports (data/numeric/visual imports are at the top of the notebook)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Train models (parameters specified) ---\n",
    "logit = make_pipeline(StandardScaler(), LogisticRegression(C=10, max_iter=10000, random_state=42))\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "svc = make_pipeline(StandardScaler(), LinearSVC(C=10, max_iter=10000, dual=False, random_state=42))\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[(\"logit\", logit.named_steps['logisticregression']), (\"rf\", rf), (\"gb\", gb)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# --- Collect training & test accuracy ---\n",
    "results = {\n",
    "    \"Logistic Regression\": [\n",
    "        accuracy_score(y_train, logit.predict(X_train)),\n",
    "        accuracy_score(y_test, logit.predict(X_test))\n",
    "    ],\n",
    "    \"Random Forest\": [\n",
    "        accuracy_score(y_train, rf.predict(X_train)),\n",
    "        accuracy_score(y_test, rf.predict(X_test))\n",
    "    ],\n",
    "    \"Gradient Boosting\": [\n",
    "        accuracy_score(y_train, gb.predict(X_train)),\n",
    "        accuracy_score(y_test, gb.predict(X_test))\n",
    "    ],\n",
    "    \"Linear SVM (scaled)\": [\n",
    "        accuracy_score(y_train, svc.predict(X_train)),\n",
    "        accuracy_score(y_test, svc.predict(X_test))\n",
    "    ],\n",
    "    \"KNN\": [\n",
    "        accuracy_score(y_train, knn.predict(X_train)),\n",
    "        accuracy_score(y_test, knn.predict(X_test))\n",
    "    ],\n",
    "    \"Naive Bayes\": [\n",
    "        accuracy_score(y_train, nb.predict(X_train)),\n",
    "        accuracy_score(y_test, nb.predict(X_test))\n",
    "    ],\n",
    "    \"MLP\": [\n",
    "        accuracy_score(y_train, mlp.predict(X_train)),\n",
    "        accuracy_score(y_test, mlp.predict(X_test))\n",
    "    ],\n",
    "    \"Ensemble (RF+Logit+GB)\": [\n",
    "        accuracy_score(y_train, ensemble.predict(X_train)),\n",
    "        accuracy_score(y_test, ensemble.predict(X_test))\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(results, index=[\"Training Accuracy\", \"Test Accuracy\"]).T\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06207309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Logistic Regression Feature Importance ---\n",
    "coef = logit.named_steps['logisticregression'].coef_[0]\n",
    "logit_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": coef,\n",
    "    \"Abs_Importance\": np.abs(coef)\n",
    "}).sort_values(by=\"Abs_Importance\", ascending=False)\n",
    "\n",
    "# --- Random Forest Feature Importance ---\n",
    "rf_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# --- Gradient Boosting Feature Importance ---\n",
    "gb_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Importance\": gb.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# --- Linear SVM Feature Importance ---\n",
    "coef_svm = svc.named_steps['linearsvc'].coef_[0]\n",
    "svc_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": coef_svm,\n",
    "    \"Abs_Importance\": np.abs(coef_svm)\n",
    "}).sort_values(by=\"Abs_Importance\", ascending=False)\n",
    "\n",
    "# --- Plot all in one window ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "\n",
    "# Logistic Regression\n",
    "logit_sorted = logit_importance.sort_values(by=\"Abs_Importance\", ascending=True)\n",
    "axes[0,0].barh(logit_sorted[\"Feature\"], logit_sorted[\"Coefficient\"], color=\"skyblue\")\n",
    "axes[0,0].set_title(\"Logistic Regression Feature Importance\")\n",
    "axes[0,0].axvline(0, color=\"black\", linewidth=0.8)\n",
    "\n",
    "# Random Forest\n",
    "rf_sorted = rf_importance.sort_values(by=\"Importance\", ascending=True)\n",
    "axes[0,1].barh(rf_sorted[\"Feature\"], rf_sorted[\"Importance\"], color=\"skyblue\")\n",
    "axes[0,1].set_title(\"Random Forest Feature Importance\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_sorted = gb_importance.sort_values(by=\"Importance\", ascending=True)\n",
    "axes[1,0].barh(gb_sorted[\"Feature\"], gb_sorted[\"Importance\"], color=\"skyblue\")\n",
    "axes[1,0].set_title(\"Gradient Boosting Feature Importance\")\n",
    "\n",
    "# Linear SVM\n",
    "svc_sorted = svc_importance.sort_values(by=\"Abs_Importance\", ascending=True)\n",
    "axes[1,1].barh(svc_sorted[\"Feature\"], svc_sorted[\"Coefficient\"], color=\"skyblue\")\n",
    "axes[1,1].set_title(\"Linear SVM Feature Importance\")\n",
    "axes[1,1].axvline(0, color=\"black\", linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collect models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": logit,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"Linear SVM (scaled)\": svc,\n",
    "    \"KNN\": knn,\n",
    "    \"Naive Bayes\": nb,\n",
    "    \"MLP\": mlp,\n",
    "    \"Ensemble (RF+Logit+GB)\": ensemble\n",
    "}\n",
    "\n",
    "# --- Run predictions for all models in one go ---\n",
    "predictions = pd.DataFrame(\n",
    "    {name: model.predict(df_predict) for name, model in models.items()},\n",
    "    index=df_predict.index\n",
    ")\n",
    "\n",
    "# --- Show results ---\n",
    "print(predictions)      # first few rows\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
