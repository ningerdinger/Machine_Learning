{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f7c45d",
   "metadata": {},
   "source": [
    "# WEO data cleaning and reshaping\n",
    "\n",
    "This notebook loads WEO data, filters to selected indicators, cleans numeric values, pivots to a panel (Country × Year) with indicators as columns, adds a global recession flag, drops incomplete years, and writes a cleaned CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57940e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# detect encoding (fallback to latin-1) and sample for delimiter sniffing\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ff0ab",
   "metadata": {},
   "source": [
    "## 1) Load CSV\n",
    "\n",
    "Detect encoding and delimiter, then read the CSV into a DataFrame. This cell expects a local `data.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Country = df['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25849a8f",
   "metadata": {},
   "source": [
    "## 2) Inspect countries\n",
    "\n",
    "Quick check to list unique country names so you can verify formatting and spot duplicates or unexpected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count columns\n",
    "print(\"Number of columns:\", df.shape[1])\n",
    "\n",
    "# list column names\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# counts of non-null values per column (good for spotting missing data)\n",
    "print(df.count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc3fac",
   "metadata": {},
   "source": [
    "## 3) Column overview\n",
    "\n",
    "Show number of columns, column names and non-null counts to locate missing-data-heavy columns that may need attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91daa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\",\"Country/Series-specific Notes\", \"Subject Notes\", \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\" ], inplace=True)\n",
    "\n",
    "codes = {\"NGDPD\",\"NGSD_NGDP\",\"PCPI\",\"TM_RPCH\",\"TX_RPCH\",\"LUR\",\"LP\",\"GGR_NGDP\",\"GGX_NGDP\",\"GGXCNL_NGDP\",\"GGSB_NPGDP\",\"GGXONLB_NGDP\",\"GGXWDN_NGDP\",\"BCA_NGDPD\"}\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699b98d",
   "metadata": {},
   "source": [
    "## 4) Filter indicators and drop unused columns\n",
    "\n",
    "We drop metadata columns and filter rows to the indicator codes of interest (NGDPD, PCPI, ...). Keep an eye on column names — mismatches will raise a KeyError."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f67bb8",
   "metadata": {},
   "source": [
    "### Cleaning and Reshaping\n",
    "\n",
    "This next code cell identifies year columns, coerces values to numeric, standardizes country names, melts the table to long format, cleans the value column, and pivots to a panel with indicators as columns.\n",
    "Run this section carefully — it performs destructive transformations on `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f98d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Identify year columns ---\n",
    "year_cols = df.columns[2:]  # after WEO Subject Code and Country\n",
    "\n",
    "# --- Step 2: Clean numeric formatting ---\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# --- Step 3: Clean country names ---\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "# --- Step 4: Melt into long format ---\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "# --- Step 5: Keep only valid year rows ---\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "# --- Step 6: Clean numeric values ---\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# --- Step 7: Pivot using WEO Subject Code as columns ---\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",   # use NGDPD, PCPI, etc.\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# --- Step 8: Finalize ---\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cca50",
   "metadata": {},
   "source": [
    "## 5) Clean numeric values, standardize country names, and reshape\n",
    "\n",
    "Steps:\n",
    "- Identify year columns and coerce them to numeric (remove commas).\n",
    "- Standardize country strings to avoid duplication (underscores, remove quotes/hyphens).\n",
    "- Melt to long format (WEO Subject Code, Country, Year, Value).\n",
    "- Keep only rows where Year looks like YYYY and ensure Value is numeric.\n",
    "- Pivot so each row is Country × Year and columns are indicators.\n",
    "\n",
    "If duplicate (Country, Year, Indicator) rows exist, the code uses `aggfunc='first'`. Inspect duplicates if results look wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year index to integer\n",
    "df_pivot.index = df_pivot.index.astype(int)\n",
    "\n",
    "# Define global recession years\n",
    "global_recession_years = [\n",
    "    1981, 1982, 1983,\n",
    "    1990, 1991, 1992, 1993,\n",
    "    2008, 2009, 2010,\n",
    "    2020, 2021\n",
    "]\n",
    "\n",
    "# Add Global_Recession column\n",
    "df_pivot[\"Global_Recession\"] = df_pivot.index.isin(global_recession_years).astype(int)\n",
    "\n",
    "df_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12287ad",
   "metadata": {},
   "source": [
    "### Modeling pipeline\n",
    "\n",
    "Prepare features `X` and target `y` and run a set of classification models (Logistic Regression, Random Forest, Gradient Boosting, SVM, KNN, Naive Bayes, MLP).\n",
    "This section performs train/test splitting and model training — ensure `df_filtered` and `X`/`y` are built from the cleaned `df_pivot` before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab620bd",
   "metadata": {},
   "source": [
    "## 6) Add global recession flag\n",
    "\n",
    "Convert the Year index to integer and add a `Global_Recession` column marking selected years. Adjust the list of recession years if your definition differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5043bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any NaN values\n",
    "df_pivot = df_pivot.dropna()\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60752f13",
   "metadata": {},
   "source": [
    "## 7) Remove incomplete rows\n",
    "\n",
    "Dropping all rows with any NaN removes years/countries that lack any indicator value. Consider imputing or relaxing this if you need more coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181860ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc624e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map countries to continents and create continent-specific DataFrames\n",
    "# Attempt to use pycountry + pycountry_convert if available, otherwise use fallback mapping\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        try:\n",
    "            lookup_name = name.replace('_', ' ')\n",
    "            country = pycountry.countries.lookup(lookup_name)\n",
    "            alpha2 = country.alpha_2\n",
    "            cc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "            continent_map = {\n",
    "                'AF': 'Africa',\n",
    "                'AS': 'Asia',\n",
    "                'EU': 'Europe',\n",
    "                'NA': 'North_America',\n",
    "                'OC': 'Oceania',\n",
    "                'SA': 'South_America',\n",
    "                'AN': 'Antarctica'\n",
    "            }\n",
    "            return continent_map.get(cc, 'Unknown')\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "except ImportError:\n",
    "    # Fallback mapping for common countries (extend as needed)\n",
    "    fallback = {\n",
    "        'United_States': 'North_America', 'Canada': 'North_America', 'Mexico': 'North_America',\n",
    "        'China': 'Asia', 'India': 'Asia', 'Japan': 'Asia', 'Afghanistan': 'Asia',\n",
    "        'Korea': 'Asia', 'Indonesia': 'Asia', 'Thailand': 'Asia', 'Vietnam': 'Asia',\n",
    "        'Germany': 'Europe', 'France': 'Europe', 'United_Kingdom': 'Europe', 'Italy': 'Europe',\n",
    "        'Spain': 'Europe', 'Russia': 'Europe', 'Turkey': 'Europe', 'Poland': 'Europe',\n",
    "        'Brazil': 'South_America', 'Argentina': 'South_America', 'Chile': 'South_America',\n",
    "        'Colombia': 'South_America', 'Peru': 'South_America', 'Venezuela': 'South_America',\n",
    "        'Australia': 'Oceania', 'New_Zealand': 'Oceania',\n",
    "        'South_Africa': 'Africa', 'Nigeria': 'Africa', 'Egypt': 'Africa', 'Zimbabwe': 'Africa',\n",
    "        'Kenya': 'Africa', 'Ethiopia': 'Africa', 'Morocco': 'Africa',\n",
    "        # Additional countries from unmapped list\n",
    "        'Albania': 'Europe', 'Algeria': 'Africa', 'Austria': 'Europe', 'Barbados': 'North_America',\n",
    "        'Belgium': 'Europe', 'Bolivia': 'South_America', 'Bosnia_and_Herzegovina': 'Europe',\n",
    "        'Bulgaria': 'Europe', 'Cabo_Verde': 'Africa', 'Costa_Rica': 'North_America',\n",
    "        'Croatia': 'Europe', 'Cyprus': 'Europe', 'Czech_Republic': 'Europe', 'Denmark': 'Europe',\n",
    "        'Dominican_Republic': 'North_America', 'Estonia': 'Europe', 'Finland': 'Europe',\n",
    "        'Hungary': 'Europe', 'Iceland': 'Europe', 'Ireland': 'Europe',\n",
    "        'Islamic_Republic_of_Iran': 'Asia', 'Israel': 'Asia', 'Jordan': 'Asia',\n",
    "        'Kazakhstan': 'Asia', 'Latvia': 'Europe', 'Lebanon': 'Asia', 'Lithuania': 'Europe',\n",
    "        'Luxembourg': 'Europe', 'Malta': 'Europe', 'Netherlands': 'Europe',\n",
    "        'North_Macedonia': 'Europe', 'Norway': 'Europe', 'Pakistan': 'Asia',\n",
    "        'Panama': 'North_America', 'Paraguay': 'South_America', 'Portugal': 'Europe',\n",
    "        'Romania': 'Europe', 'Saudi_Arabia': 'Asia', 'Serbia': 'Europe', 'Seychelles': 'Africa',\n",
    "        'Slovak_Republic': 'Europe', 'Slovenia': 'Europe', 'Sweden': 'Europe',\n",
    "        'Switzerland': 'Europe', 'Syria': 'Asia', 'Taiwan_Province_of_China': 'Asia',\n",
    "        'Trinidad_and_Tobago': 'North_America', 'Türkiye': 'Europe', 'Uruguay': 'South_America'\n",
    "    }\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        return fallback.get(name.replace(' ', '_'), 'Unknown')\n",
    "\n",
    "# Add Continent column to df_filtered\n",
    "df_filtered_copy = df_filtered.copy()\n",
    "df_filtered_copy['Continent'] = df_filtered_copy['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "# Create continent-specific DataFrames as globals\n",
    "continent_names = sorted(df_filtered_copy['Continent'].dropna().unique())\n",
    "created = []\n",
    "\n",
    "for cont in continent_names:\n",
    "    safe = str(cont).replace(' ', '_')\n",
    "    df_name = f'df_{safe}'\n",
    "    subset = df_filtered_copy[df_filtered_copy['Continent'] == cont].copy()\n",
    "    # Remove the Continent column from the subset\n",
    "    subset = subset.drop(columns=['Continent'])\n",
    "    globals()[df_name] = subset\n",
    "    created.append((cont, df_name, len(subset)))\n",
    "\n",
    "# Print summary\n",
    "print('Created continent-specific DataFrames:')\n",
    "for cont, var_name, rows in created:\n",
    "    print(f' - {cont}: {var_name} (rows: {rows})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d90542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[df_pivot.index <= 2024]\n",
    "df_filtered\n",
    "\n",
    "# Assuming df_filtered is your dataframe with Year as index\n",
    "X = df_filtered.drop(columns=[\"Global_Recession\", \"Country\"])  # features\n",
    "y = df_filtered[\"Global_Recession\"]                           # target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Exploratory Plots ---\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df_filtered.drop(columns=[\"Country\"]).corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap of Economic Indicators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports (data/numeric/visual imports are at the top of the notebook)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9822947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "coefficients = model.coef_[0]\n",
    "features = X_train.columns\n",
    "\n",
    "logit_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": coefficients,\n",
    "    \"Abs_Importance\": np.abs(coefficients)\n",
    "}).sort_values(by=\"Abs_Importance\", ascending=False)\n",
    "\n",
    "print(\"Logistic Regression Feature Importance:\")\n",
    "print(logit_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(logit_importance[\"Feature\"], logit_importance[\"Coefficient\"])\n",
    "plt.xlabel(\"Coefficient (Impact on Log-Odds)\")\n",
    "plt.title(\"Logistic Regression Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(rf_importance[\"Feature\"], rf_importance[\"Importance\"])\n",
    "plt.xlabel(\"Importance (Mean Decrease in Impurity)\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Gradient Boosting:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, gb.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "gb_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": gb.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Feature Importance:\")\n",
    "print(gb_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(gb_importance[\"Feature\"], gb_importance[\"Importance\"])\n",
    "plt.xlabel(\"Importance (Mean Decrease in Impurity)\")\n",
    "plt.title(\"Gradient Boosting Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed39669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train Linear SVM\n",
    "linear_svm = LinearSVC(random_state=42, max_iter=5000)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "svm_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": linear_svm.coef_[0],\n",
    "    \"Abs_Importance\": np.abs(linear_svm.coef_[0])\n",
    "}).sort_values(by=\"Abs_Importance\", ascending=False)\n",
    "\n",
    "print(\"Linear SVM Feature Importance:\")\n",
    "print(svm_importance)\n",
    "\n",
    "# Accuracy on train and test sets\n",
    "train_acc = accuracy_score(y_train, linear_svm.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746af438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- KNN ---\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"KNN:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "train_acc = accuracy_score(y_train, knn.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"KNN Test Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "# --- Naive Bayes ---\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Naive Bayes:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "train_acc = accuracy_score(y_train, nb.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naive Bayes Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Naive Bayes Test Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "# --- MLP ---\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"MLP:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "train_acc = accuracy_score(y_train, mlp.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"MLP Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"MLP Test Accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance for models that support it\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
    "\n",
    "sns.barplot(x=\"Abs_Importance\", y=\"Feature\", data=logit_importance, ax=axes[0])\n",
    "axes[0].set_title(\"Logistic Regression Coefficients\")\n",
    "\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=rf_importance, ax=axes[1])\n",
    "axes[1].set_title(\"Random Forest Importance\")\n",
    "\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=gb_importance, ax=axes[2])\n",
    "axes[2].set_title(\"Gradient Boosting Importance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177248b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print accuracies for all models ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression\n",
    "logit_train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "logit_test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "# Random Forest\n",
    "rf_train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "rf_test_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_train_acc = accuracy_score(y_train, gb.predict(X_train))\n",
    "gb_test_acc = accuracy_score(y_test, gb.predict(X_test))\n",
    "\n",
    "# Linear SVM\n",
    "svm_train_acc = accuracy_score(y_train, linear_svm.predict(X_train))\n",
    "svm_test_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
    "\n",
    "# KNN\n",
    "knn_train_acc = accuracy_score(y_train, knn.predict(X_train))\n",
    "knn_test_acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_train_acc = accuracy_score(y_train, nb.predict(X_train))\n",
    "nb_test_acc = accuracy_score(y_test, nb.predict(X_test))\n",
    "\n",
    "# MLP\n",
    "mlp_train_acc = accuracy_score(y_train, mlp.predict(X_train))\n",
    "mlp_test_acc = accuracy_score(y_test, mlp.predict(X_test))\n",
    "\n",
    "print(\"Model Accuracies:\")\n",
    "print(f\"Logistic Regression - Train: {logit_train_acc:.3f}, Test: {logit_test_acc:.3f}\")\n",
    "print(f\"Random Forest       - Train: {rf_train_acc:.3f}, Test: {rf_test_acc:.3f}\")\n",
    "print(f\"Gradient Boosting   - Train: {gb_train_acc:.3f}, Test: {gb_test_acc:.3f}\")\n",
    "print(f\"Linear SVM          - Train: {svm_train_acc:.3f}, Test: {svm_test_acc:.3f}\")\n",
    "print(f\"KNN                 - Train: {knn_train_acc:.3f}, Test: {knn_test_acc:.3f}\")\n",
    "print(f\"Naive Bayes         - Train: {nb_train_acc:.3f}, Test: {nb_test_acc:.3f}\")\n",
    "print(f\"MLP                 - Train: {mlp_train_acc:.3f}, Test: {mlp_test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a219de",
   "metadata": {},
   "source": [
    "## 8) Export\n",
    "\n",
    "Write the cleaned panel to CSV for downstream analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b757ad",
   "metadata": {},
   "source": [
    "### Results and Export\n",
    "\n",
    "After running the cleaning and modeling sections, inspect `df_pivot`/`df_filtered` outputs (head, tail, non-null counts) before exporting. The following export cell writes `weo_data_clean.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to CSV\n",
    "df_pivot.to_csv(\"weo_data_clean.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
